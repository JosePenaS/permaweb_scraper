name: Scrape Permaweb-Journal

on:
  # run every day at 06:00 UTC ≈ 02:00 Chile time
  schedule:
    - cron:  '0 6 * * *'
  workflow_dispatch:  # adds a manual “Run workflow” button

jobs:
  scrape:
    runs-on: ubuntu-latest

    # Supabase credentials pulled from Settings → Secrets → Actions
    env:
      SUPABASE_HOST: ${{ secrets.SUPABASE_HOST }}
      SUPABASE_PORT: ${{ secrets.SUPABASE_PORT }}
      SUPABASE_DB:   ${{ secrets.SUPABASE_DB }}
      SUPABASE_USER: ${{ secrets.SUPABASE_USER }}
      SUPABASE_PWD:  ${{ secrets.SUPABASE_PWD }}

    steps:
      - uses: actions/checkout@v4

      - uses: r-lib/actions/setup-r@v2   # installs R 4.x

      # system headers for RPostgres, rvest → xml2, and curl/openssl
      - name: Install system libraries
        run: |
          sudo apt-get update -y
          sudo apt-get install -y \
            libpq-dev libxml2-dev libcurl4-openssl-dev libssl-dev

      # core R packages your scraper needs
      - name: Install R dependencies
        run: |
          Rscript -e 'install.packages(
            c("rvest","dplyr","stringr","lubridate","DBI","RPostgres"),
            repos = "https://cloud.r-project.org"
          )'

      # ────────── DEBUG: show raw bytes of SUPABASE_HOST ──────────
      - name: Debug host value
        run: |
          printf 'Host bytes: '
          printf '%s' "$SUPABASE_HOST" | od -An -t x1
      # ─────────────────────────────────────────────────────────────

      - name: Run scraper
        run: Rscript permaweb_scraper2.R   # change if your script has a different name
